{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scala_spark 统计每日访问量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "local"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@79cef4c2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@79cef4c2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "      .builder()\n",
    "      .master(\"local[2]\")\n",
    "      .appName(\"WindowTest\")\n",
    "      .getOrCreate()\n",
    "//    import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建模拟用户访问表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.functions._\n",
    "// 聚合函数都来自于func\n",
    "import org.apache.spark.sql.functions.row_number\n",
    "import org.apache.spark.sql.functions.dense_rank\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "// pyspark:pyspark.sql.window.Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UserAcessLog = Array(2019-05-29,aa,45, 2019-05-29,bb,45, 2019-05-29,aa,45, 2019-05-29,cc,45, 2019-05-30,aa,35, 2019-05-30,ee,35, 2019-05-30,aa,35, 2019-05-30,aa,35)\n",
       "LogStrRDD = MapPartitionsRDD[2] at map at <console>:43\n",
       "schema = StructType(StructField(date,StringType,true), StructField(userid,StringType,true), StructField(click,IntegerType,true))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(date,StringType,true), StructField(userid,StringType,true), StructField(click,IntegerType,true))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val UserAcessLog = Array(\"2019-05-29,aa,45\",\"2019-05-29,bb,45\",\n",
    "                         \"2019-05-29,aa,45\",\"2019-05-29,cc,45\",\n",
    "                         \"2019-05-30,aa,35\",\"2019-05-30,ee,35\",\n",
    "                         \"2019-05-30,aa,35\",\"2019-05-30,aa,35\"\n",
    "                        )\n",
    "val LogStrRDD = sc.parallelize(UserAcessLog,1)\n",
    "                .filter(line => if(line.split(\",\")(2).length >0) true else false)\n",
    "                .map(line=> \n",
    "                     Row(line.split(\",\")(0), \n",
    "                         line.split(\",\")(1),\n",
    "                         line.split(\",\")(2).toInt))\n",
    "\n",
    "val schema = StructType(Array(StructField(\"date\",StringType),\n",
    "                             StructField(\"userid\",StringType),\n",
    "                             StructField(\"click\",IntegerType))\n",
    "                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([2019-05-29,aa,45], [2019-05-29,bb,45], [2019-05-29,aa,45], [2019-05-29,cc,45], [2019-05-30,aa,35])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogStrRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogDF = [date: string, userid: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[date: string, userid: string ... 1 more field]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val LogDF = spark.sqlContext.createDataFrame(LogStrRDD,schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+\n",
      "|      date|userid|click|\n",
      "+----------+------+-----+\n",
      "|2019-05-29|    aa|   45|\n",
      "|2019-05-29|    bb|   45|\n",
      "|2019-05-29|    aa|   45|\n",
      "|2019-05-29|    cc|   45|\n",
      "|2019-05-30|    aa|   35|\n",
      "|2019-05-30|    ee|   35|\n",
      "|2019-05-30|    aa|   35|\n",
      "|2019-05-30|    aa|   35|\n",
      "+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LogDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+---------+\n",
      "|      date| uv|click_sum|\n",
      "+----------+---+---------+\n",
      "|2019-05-29|  3|      180|\n",
      "|2019-05-30|  2|      140|\n",
      "+----------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LogDF.groupBy(\"date\").agg(\n",
    "    countDistinct(\"userid\").as(\"uv\"),\n",
    "    sum(\"click\").alias(\"click_sum\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 开窗函数：分组求topn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// 这种写法在Pyspark可以，Python增加字段只能用withColumn\n",
    "// LogDF.select(\"date\",\"userid\",LogDF(\"click\")+10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rankSpec = org.apache.spark.sql.expressions.WindowSpec@1e3d2eb7\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.expressions.WindowSpec@1e3d2eb7"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rankSpec = Window.partitionBy(\"date\").orderBy(LogDF(\"click\").desc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+----+\n",
      "|      date|userid|click|rank|\n",
      "+----------+------+-----+----+\n",
      "|2019-05-29|    aa|   45|   1|\n",
      "|2019-05-29|    bb|   45|   1|\n",
      "|2019-05-29|    aa|   45|   1|\n",
      "|2019-05-29|    cc|   45|   1|\n",
      "|2019-05-30|    aa|   35|   1|\n",
      "|2019-05-30|    ee|   35|   1|\n",
      "|2019-05-30|    aa|   35|   1|\n",
      "|2019-05-30|    aa|   35|   1|\n",
      "+----------+------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LogDF.withColumn(\"rank\", dense_rank().over(rankSpec)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// 这种写法在Pyspark可以，Python增加字段只能用withColumn\n",
    "/*\n",
    "*LogDF.select(\"date\",\"userid\",\"click\",\n",
    "*             dense_rank().over(\n",
    "*                 Window.partitionBy(\"date\")\n",
    "*                     .orderBy(\"click\")).alias(\"aa\"))\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark2.1.3 - Scala",
   "language": "scala",
   "name": "spark2.1.3_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
